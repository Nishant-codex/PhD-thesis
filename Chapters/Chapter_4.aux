\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{140}{section*.95}\protected@file@percent }
\citation{koch1999complexity,habashy2024adapting}
\citation{london2010sensitivity,brunel2003firing,wang2020heterogeneity}
\citation{cavanagh2020diversity}
\citation{runyan2017distinct}
\citation{mcnaughton2006path}
\citation{hasson2008hierarchical,chu2020long}
\citation{lukovsevivcius2009reservoir}
\citation{jaeger2001echo}
\citation{schrauwen2007overview}
\citation{iacob2022distance}
\citation{izhikevich2006polychronization}
\citation{iacob2022distance,soriano2014delay,perez2021neural }
\@writefile{toc}{\contentsline {section}{Introduction}{141}{section*.96}\protected@file@percent }
\citation{london2010sensitivity,nam2017diversity,zhou2023temporal}
\@writefile{toc}{\contentsline {section}{Methods}{144}{section*.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Network Design}{144}{subsection*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Echo State networks }{144}{subsubsection*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Distance based delay networks}{144}{subsubsection*.100}\protected@file@percent }
\citation{iacob2022distance}
\@writefile{toc}{\contentsline {subsection}{Task}{145}{subsection*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NARMA-30}{145}{subsubsection*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Evaluation}{146}{subsubsection*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mackey-Glass}{146}{subsubsection*.104}\protected@file@percent }
\citation{hansen2006the-cma-evoluti}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation}{147}{subsubsection*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training}{147}{subsection*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CMA-ES Optimization}{147}{subsubsection*.107}\protected@file@percent }
\citation{boedecker2012information}
\citation{boedecker2012information}
\@writefile{toc}{\contentsline {subsubsection}{Readout training}{148}{subsubsection*.108}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Lyapunov exponent}{148}{subsection*.109}\protected@file@percent }
\newlabel{eq:lyapuno}{{4.9}{148}{Lyapunov exponent}{equation.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{Dimensionality}{149}{subsection*.110}\protected@file@percent }
\citation{jaeger2001short}
\@writefile{toc}{\contentsline {subsection}{Linear Memory Capacity}{150}{subsection*.111}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Results}{150}{section*.112}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Task Performance }{151}{subsection*.114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NARMA 30}{151}{subsubsection*.115}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {Reservoir architecture and heterogeneity design in DDNs and ESNs:} \textbf  {(a)} DDNs were made heterogeneous by introducing heterogeniety in delays ($\mitalpha $), cluster heterogeneity is designed by sampling delays from different distributions for each cluster, similarly, network heterogeneity is designed by sampling delays from a single distribution. \textbf  {(b)} same heterogeneity designed for ESNs. It can also be seen that the key difference between ESNs and DDNs are the introduction of delays between units in case of DDNs. }}{152}{figure.caption.113}\protected@file@percent }
\newlabel{fig:reservoir-arch}{{4.1}{152}{\textbf {Reservoir architecture and heterogeneity design in DDNs and ESNs:} \textbf {(a)} DDNs were made heterogeneous by introducing heterogeniety in delays ($\alpha $), cluster heterogeneity is designed by sampling delays from different distributions for each cluster, similarly, network heterogeneity is designed by sampling delays from a single distribution. \textbf {(b)} same heterogeneity designed for ESNs. It can also be seen that the key difference between ESNs and DDNs are the introduction of delays between units in case of DDNs}{figure.caption.113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mackey-Glass}{153}{subsubsection*.116}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {Network performance for DDNs and ESNs with or without heterogeneity}: \textbf  {(a)} Line plot shows the perforamce in terms of NRMSE over generation during CMA-ES optimization for DDNs and ESNs during NARMA 30 task. }}{154}{figure.caption.117}\protected@file@percent }
\newlabel{fig:eval}{{4.2}{154}{\textbf {Network performance for DDNs and ESNs with or without heterogeneity}: \textbf {(a)} Line plot shows the perforamce in terms of NRMSE over generation during CMA-ES optimization for DDNs and ESNs during NARMA 30 task}{figure.caption.117}{}}
\@writefile{toc}{\contentsline {subsection}{Stability and Dimensionality}{155}{subsection*.119}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Maximum Lyapunov exponents}{155}{subsubsection*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality}{156}{subsubsection*.121}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Linear Memory capacity}{157}{subsection*.123}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Dynamics based on Lyapunov exponent and dimensionality of optimized networks:} \textbf  {(a-b)} shows the Lyapunov exponents of the optimized networks while performing the NARMA 30 (blue background) and Mackey-Glass (red background). \textbf  {(c-d)} shows the network dimensionality while performing the NARMA 30 (blue background) and Mackey-Glass timeseries prediction tasks (red background).}}{158}{figure.caption.122}\protected@file@percent }
\newlabel{fig:lyapunov}{{4.3}{158}{\textbf {Dynamics based on Lyapunov exponent and dimensionality of optimized networks:} \textbf {(a-b)} shows the Lyapunov exponents of the optimized networks while performing the NARMA 30 (blue background) and Mackey-Glass (red background). \textbf {(c-d)} shows the network dimensionality while performing the NARMA 30 (blue background) and Mackey-Glass timeseries prediction tasks (red background)}{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf  {Linear Memory Capacity of optimized networks}: \textbf  {(Left)} Linear memory capacity of the networks optimized for NARMA 30 task for different delays. \textbf  {(Right)} Linear memory capacity of the networks optimized for Mackey-Glass time series prediction task for different delays. }}{159}{figure.caption.124}\protected@file@percent }
\newlabel{fig:MC}{{4.4}{159}{\textbf {Linear Memory Capacity of optimized networks}: \textbf {(Left)} Linear memory capacity of the networks optimized for NARMA 30 task for different delays. \textbf {(Right)} Linear memory capacity of the networks optimized for Mackey-Glass time series prediction task for different delays}{figure.caption.124}{}}
\citation{iacob2022distance}
\citation{tanaka2022reservoir}
\citation{Lundqvist2016gamma,Runyan2017timescales,Cavanagh2020diverse}
\@writefile{toc}{\contentsline {section}{Discussion}{160}{section*.125}\protected@file@percent }
\bibstyle{apalike}
\bibdata{All_bibtex}
\bibcite{boedecker2012information}{{1}{2012}{{Boedecker et~al.}}{{}}}
\bibcite{brunel2003firing}{{2}{2003}{{Brunel and Wang}}{{}}}
\bibcite{Cavanagh2020diverse}{{3}{2022}{{Cavanagh et~al.}}{{}}}
\bibcite{cavanagh2020diversity}{{4}{2020}{{Cavanagh et~al.}}{{}}}
\bibcite{chu2020long}{{5}{2020}{{Chu et~al.}}{{}}}
\bibcite{habashy2024adapting}{{6}{2024}{{Habashy et~al.}}{{}}}
\bibcite{hansen2006the-cma-evoluti}{{7}{2006}{{Hansen et~al.}}{{}}}
\bibcite{hasson2008hierarchical}{{8}{2008}{{Hasson et~al.}}{{}}}
\bibcite{iacob2022distance}{{9}{2022}{{Iacob et~al.}}{{}}}
\bibcite{izhikevich2006polychronization}{{10}{2006}{{Izhikevich}}{{}}}
\bibcite{jaeger2001short}{{11}{2001a}{{Jaeger}}{{}}}
\bibcite{jaeger2001echo}{{12}{2001b}{{Jaeger}}{{}}}
\bibcite{koch1999complexity}{{13}{1999}{{Koch and Laurent}}{{}}}
\bibcite{london2010sensitivity}{{14}{2010}{{London et~al.}}{{}}}
\bibcite{lukovsevivcius2009reservoir}{{15}{2009}{{Lukoševičius and Jaeger}}{{}}}
\bibcite{Lundqvist2016gamma}{{16}{2016}{{Lundqvist et~al.}}{{}}}
\bibcite{mcnaughton2006path}{{17}{2006}{{McNaughton et~al.}}{{}}}
\bibcite{nam2017diversity}{{18}{2017}{{Nam et~al.}}{{}}}
\bibcite{Runyan2017timescales}{{19}{2017a}{{Runyan et~al.}}{{}}}
\bibcite{runyan2017distinct}{{20}{2017b}{{Runyan et~al.}}{{}}}
\bibcite{schrauwen2007overview}{{21}{2007}{{Schrauwen et~al.}}{{}}}
\bibcite{soriano2014delay}{{22}{2014}{{Soriano et~al.}}{{}}}
\bibcite{tanaka2022reservoir}{{23}{2022}{{Tanaka et~al.}}{{}}}
\bibcite{wang2020heterogeneity}{{24}{2020}{{Wang et~al.}}{{}}}
\bibcite{zhou2023temporal}{{25}{2023}{{Zhou et~al.}}{{}}}
\@setckpt{Chapters/Chapter_4}{
\setcounter{page}{165}
\setcounter{equation}{13}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{NAT@ctr}{25}
\setcounter{KVtest}{0}
\setcounter{parentequation}{0}
\setcounter{nlinenum}{0}
\setcounter{float@type}{4}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{tblrcount}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{section@level}{0}
\setcounter{Item}{18}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{4}
}
