\fancyhf{}
\fancyhead[C]{Chapter 5. General Discussion}% <- added
\fancyfoot[R]{\thepage\ifodd\value{page}\else\hfill\fi}
%\fancyhead[L]{\ifodd\value{page}\relax\else\hfill\fi Ch \thechapter}
%\renewcommand\headrulewidth{0pt}% default ist .4pt
\renewcommand{\plainheadrulewidth}{.4pt}% default is 0pt


\section{Summary}

The broad aim of this thesis was to shed light on neuronal functional heterogeneity based on context and neuromodulation. The thesis addresses three questions: (1) How does input context influence functional classification of neurons and which feature(s) are the most informative about neuronal heterogeneity? (2) How does neuromodulation alter the functional landscape of neurons? (3) Can intrinsic timescale heterogeneity improve performance and memory in reservoir computing systems? 

\textbf{Chapter 2} takes on the first research question. It challenges the long-standing assumption in systems neuroscience that neuronal functional identity is fixed. Through direct comparison of neuronal responses to different input regimes, I demonstrated that the functional identity of neurons is context-dependent, varying with the nature of the input that neurons receive. Moreover, I provide a framework to compare the electrophysiological attributes commonly used in functional classification, such as action potential and passive biophysical attributes, which I collectively refer to as functional feature space. By exploiting the high-dimensional functional feature space of neurons and comparing the individual feature sets which are subspaces of the aforementioned feature space, showing that the linear input filter accounts for a greater proportion of variance across neurons, we show that STA is more informative and discriminative attribute set for functional classification than adaptation, action potential and passive biophysical attribute sets.

In \textbf{Chapter 3}, I take on the second research question. I show that neuromodulation alters information transfer in a cell-type and agonist-specific manner. In the previous chapter I established the role of input in functional classification and laid out a method to study and compare heterogeneity in a high dimensional functional space. Here, I show the influence of neuromodulation on this high-dimensional functional space. In this chapter, I show that neuromodulatory changes in functional properties are not uniformly distributed across the neuronal population; instead, neuromodulation induces shifts in multiple functional properties in a coordinated, subpopulation-specific manner. This adds further complexity to functional classification by showing that identity is not only input-dependent but also dynamically reshaped by neuromodulation.

Finally in \textbf{Chapter 4}, I extend the investigation of functional heterogeneity in neurons to artificial systems, attempting to answer the third research question. In this chapter I examine how intrinsic timescale heterogeneity affects performance in reservoir computing networks. I demonstrate that Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs) exhibit improved task performance and memory capacity when composed of units with heterogeneous time constants. This supports the idea that modular heterogeneity (sub-populations with decay parameters from distinct distributions) in timescales can enhance computational capability both in biological and artificial systems.          

In the following sections, I will synthesize the key findings of this thesis and discuss their broader implications in a conjoint manner 

\section{Towards dynamic functional identity of single neurons}

\subsection{Input dependence challenges current taxonomies}

Most serious efforts towards understanding neuronal heterogeneity have been standing on this assumption that neuronal functional identity is static (\cite{fishell2013neuron, huang2019diversity, markram2004interneurons, mukamel2019perspectives, nelson2006problem, petilla2008petilla, sanes2015types, seung2014neuronal, somogyi2005defined, yuste2020community, zeng2022cell, zeng2017neuronal,komendantov_quantitative_2019}). In \textbf{Chapter 2} I challenge this foundational assumption by showing that neuronal functional identity is context dependent. Despite large-scale classification efforts such as by the Allen Brain Institute (\cite{gouwens2019classification, gouwens2020integrated}), the dependence of electrophysiological types (e-types) on the nature of synaptic input remains largely overlooked. Currently, multi-modal classification efforts combine morphological, electrophysiological and transcriptomic markers together to define cell class. In these studies, a static step protocol is used to extract electrophysiological features. This thesis challenges the methodology underlying these classification results and demonstrates their input-dependence. Here, I provide evidence that neuronal classification is input-dependent, and that functional identity shifts under different stimulus regimes. This insight calls for a critical reassessment of current classification schemes such as by the Allen Institute (\cite{gouwens2019classification, gouwens2020integrated}) in systems neuroscience. These findings advocate for a paradigm shift: functional classification should take the context into account, because different contexts (input and neuromodulation) will lead to different classification results. Achieving this requires a renewed effort in data collection one that prioritizes dynamic stimulation paradigms as well as neuromodulatory state of the network. While more resource-intensive, this approach offers a deeper and more meaningful classification for modeling brain computation.


\subsection{Functional value of heterogeneity at the population level}

In the previous section I described the role of dynamical functional identity in deciding functional classification, but this just one single aspect of the results in \textbf{Chapter 2}. The effect of a context dependent functional identity of single neurons needs to be understood on broader scale, at the level of neural circuits. In this section, I aim to highlight the circuit level interpretation of functional heterogeneity. 

In\textbf{ Chapter 2}, the STA emerged as most informative for capturing population-level heterogeneity among the four attribute sets (Action Potential, Passive biophysical, Adaptation Current and STA, see Chapter 1). This indicates that even within a single cortical region, the somatosensory cortex, neurons differ substantially in their linear input filters. The clustering results show multiple subgroups of neurons with distinct STAs which implies a level of functional modularity, where neurons are tuned to distinct input features. This specialization may provide a basis for modularity in cortical computation and aid cognitive flexibility (\cite{wu2025neural,hutt2023intrinsic}). Also, work by \cite{buonomano2009state} suggest that activity of a neuron in a network is a complex interaction of the network state at any given instant and the incoming stimuli. This view is completely supported by our findings, as such suggesting that functional role of a neuron in a network is not fixed but is dynamically changing by the input it receives from other neurons in the network and state of neuromodulation. It is likely that this dynamic functional heterogeneity would be even more pronounced when comparing neurons across different cortical areas. These findings argue that cortical models must incorporate variability in feature selectivity of neurons to accurately reflect population-level dynamics. 

Having established that STA reveals intrinsic functional heterogeneity, we next asked how this heterogeneity and functional classification can be reconfigured by neuromodulatory signals. In \textbf{Chapter 3}, I use the established framework to study neuromodulatory effects, focusing on dopaminergic (D1, D2) and cholinergic (muscarinic M1) receptor activation. Neuromodulation altered both the amount of transmitted information and the classification of neurons across the four attribute sets. I show that activation of the aforementioned receptors reshaped the correlation structure among the PB, AP, AC and STA attribute sets, demonstrating that neuromodulation does not act on one single attribute in isolation, but reorganizes the functional feature space (a collection of neuronal functional attributes) of neurons. Notably, these effects were heterogeneous: subpopulations of neurons responded differently to each receptor agonist, indicating that neuromodulators function as fine-tuned control mechanisms that selectively modulate specific circuit subsets (\cite{ogawa2023multitasking,salvan2023serotonin,gast2024neural}). This differential, subpopulation-specific reorganization suggests that neuromodulation operates by exploiting the circuit’s intrinsic heterogeneity to facilitate rapid, context-sensitive switching between different population-level states.

There has been an extensive body of literature that discuss population level control via neuromodulation (\cite{Agnati1995,Fuxe2016,Herrington2016,CervantesSandoval2017,Little2013,Zilles2009,marder2014neuromodulation}). Neuromodulators work thorough volume transmission (\cite{Fuxe2016}), through which they control groups of neurons. The results in this thesis provide a direct evidence of this. We show a detailed view of 
how sub-populations of neurons alter different functional attributes as a result of specific receptor activation in the somatosensory cortex and the change in correlation structure (i.e. coordination) between functional attribute sets for a neuronal population. 

The above discussion details the mechanisms underlying neuronal identity, underscoring the shift toward a dynamic model. Our findings established that functional identity is fundamentally context-dependent on the input, and that the intrinsic functional heterogeneity captured by the linear input filter (STA) provides a foundation for functional modularity or clusters of neurons. Crucially, we demonstrated that neuromodulation operates by exploiting this intrinsic heterogeneity, selectively reorganizing the functional feature space; specifically by reshaping the correlation structure between multiple attribute sets in order to facilitate rapid, context-sensitive switching of population dynamics. Ultimately, these results underscore that a neuron’s functional role is not fixed, but emerges dynamically from the combined influence of incoming stimuli and the overall neuromodulatory network state, arguing for the necessity of incorporating dynamic functional heterogeneity into future circuit models.

\subsection{Introduction of novel analytical methods to study functional reorganization and high-dimensional correlations}

Addressing the dynamic nature of neuronal functional classification and heterogeneity required the application of analytical tools not traditionally used in electrophysiological studies. To this end, I employed unsupervised machine learning techniques specifically, Louvain clustering combined with UMAP for dimensionality reduction, and Multi-set Correlation and Factor Analysis (MCFA) (\cite{brown2023multiset}) for feature-space variance comparison.

While Louvain+UMAP has been previously applied to waveform clustering (\cite{lee2021non}), its application to electrophysiological feature space across distinct input contexts is novel. This method is useful for classifying neurons based on high-dimensional attribute sets extracted from the recordings. It is an extremely suitable method for this particular use case due to its ability to directly extract a high-dimensional graph structure using UMAP based on the high dimensional attribute sets. This extracted graph is then used for clustering instead of a low dimensional projection of the original attribute set using a dimensionality reduction method such as PCA, which results in loss of information during projection. And also, due to the fact that Louvain is an unsupervised method which is based on optimizing a modularity index with a robust hyperparameter selection criteria, and doesn't require manually selecting numbers of clusters. In this thesis, I leveraged this framework to directly compare neuronal classifications under different stimulation regimes (e.g., step current vs. frozen noise), revealing context-dependent shifts in functional identity.

A persistent challenge in electrophysiological classification is the lack of consensus on which features most effectively capture neuronal heterogeneity. To move beyond arbitrary or limited feature selection, I employed MCFA to quantify the correlation structure across high-dimensional attribute sets which are sets of functional features extracted from neural recordings (see \textbf{Chapter 1}). This method allowed us to compare these attribute sets by resolving the structure into shared variance (variance common across sets) and private variance (variance unique to a specific set). The private variance structure allows us to identify which set is the most useful for characterizing functional diversity. Furthermore, we used MCFA to assess how the correlation structure between attribute sets changes under neuromodulatory influence, demonstrating that neuromodulation alters the network state by coordinating or reorganizing the functional attributes.     

Although UMAP+Louvain clustering and MCFA are established tools in genomics and single-cell omics (\cite{brown2023multiset}), their adoption in systems neuroscience remains limited. This thesis demonstrates their applicability to electrophysiological data and advocates for their broader use in analyzing the high-dimensional functional landscape of neurons. With more comprehensive datasets, these methods are likely to yield deeper, more robust insights into functional heterogeneity and neuronal classification. We also expect these methods to be adapted more widely for many other problem domains in neuroscience.

\section{Computational modeling implications}

I showed in \textbf{Chapter 2} and \textbf{Chapter 3} that cortical neurons are heterogeneous in their functional properties and in the way they respond to neuromodulation using single unit recordings. In \textbf{Chapter 4}, I tried to synthesize this observation in artificial system. In order to study the advantage it could have on computational properties of reservoir computing systems. We chose RC networks for studying the effect of intrinsic heterogeneity because it offers a much more controlled framework for varying parameters for individual units ($\alpha$ or decay parameter) compared to much more complicated networks such as spiking neural networks. Moreover, training RC networks on benchmark tasks are much simpler compared to spiking neural networks.    

\subsection{Heterogeneity Improves Reservoir Performance}

In \textbf{Chapter 4}, I demonstrate that timescale heterogeneity enhances the performance of both Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs) on NARMA-30 and Mackey-Glass tasks, particularly in terms of memory capacity and task accuracy. However, we found that the relationship is not monotonous: the most heterogeneous architecture, that is the DNNs/ESNs with heterogeneous cluster do not always yield the best performance. Previous literature has supported the use of modular heterogeneity in artificial networks, where a modular structure improves task performance (\cite{yang2024brain,manneschi2021exploiting}).   

DDNs already incorporate heterogeneous inter-unit delays by design (\cite{iacob2022distance}), which contributes to a more distributed memory profile compared to ESNs (\cite{iacob2022distance}). This structural feature gives DDNs a baseline advantage in tasks requiring temporal integration. Adding intrinsic decay (timescale) heterogeneity further enhances performance, as shown in the memory capacity analysis but only up to a point. Beyond a certain level of heterogeneity, the gains diminish or reverse. The improvement in performance showed by heterogeneous networks result proves the hypothesis by (\cite{tanaka2022reservoir}) which proposes that a reservoir with multiple times scales in the individual units will make the reservoir dynamic richer and would aid in learning temporal tasks. However our results underscores another key insight: the benefits of timescale heterogeneity are task-dependent, and excessive heterogeneity can, in some cases, degrade performance. 

The improved performance of ESNs and DDNs with modular timescale heterogeneity on benchmark tasks (namely NARMA and Mackey-Glass tasks) reinforces the need to define the optimal level of heterogeneity. This beneficial effect aligns with extensive literature showing that introducing timescale heterogeneity improves encoding, memory, and temporal task performance in spiking neural networks (\cite{mejias_optimal_2012,zheng2024temporal,tripathy_intermediate_2013,gjorgjieva2016computational,gjorgjieva_intrinsic_2014,duarte_leveraging_2019,chakraborty_heterogeneous_2022,zeldenrust_efficient_2021,gast2024neural,marsat2010neural}). For instance, while moderate timescale heterogeneity improved performance in both ESNs and DDNs on the short-term memory-intensive NARMA-30 task, excessive heterogeneity introduced instability on the chaotic Mackey-Glass prediction task, reducing predictive accuracy. Furthermore, heterogeneous networks have been shown to be more robust and stable than homogeneous networks (\cite{perez-nieves_neural_2021}). Collectively, these results highlight that while intrinsic timescale heterogeneity is a clear benefit for both ESNs and DDNs, its advantage is fundamentally task-dependent, underscoring the necessity of calibrating the degree of heterogeneity to the specific computational demands of the system. 


Interestingly, both homogeneous cluster (clusters with singular decay value) and heterogeneous cluster (clusters with decay parameters sampled from a distinct distribution) architectures outperformed fully homogeneous or fully heterogeneous networks across tasks. We also saw that networks heterogeneous clusters showed contracting dynamics across ESNs and DDNs. This suggests that structured heterogeneity, organized into modular subgroups, offers an optimal balance between flexibility (by diversifying time scales for memory) and control (by keeping the network dynamics contracting). This result connects well with the findings in \textbf{Chapter 2-3}, where I show that neuronal populations show modularity in terms of modulation and in their intrinsic attributes. These findings emphasize that while intrinsic timescale heterogeneity is beneficial, its impact depends critically on its structure and distribution within the network and the requirements of the task the network needs to learn.


\subsection{Limitations and Methodological Constraints}

In this thesis I have used rigorous analyses and interpretation to show evidence against a static neuronal identity. Moreover, I show the effect of neuromodulation on neuronal functional space and effect of heterogeneity on the performance of reservoir computing networks. Even though the results are robust, it is important to acknowledge limitations pertaining to the methodology and data. 

First, while the frozen noise stimulation provided a richer input than the step-and-hold protocols, the overall number of cells recorded under both conditions are limited. Consequently, the statistical power for detecting more subtle subpopulation-level effect is limited. This also applies to the recordings for each agonist condition where only a subset was recorded under both control and agonist conditions. 

Second, while the classification results revealed clear context- and input-dependent changes in neuronal identity, these findings are currently restricted to neurons within the somatosensory cortex. It remains to be seen whether similar dynamic reconfigurations occur in other cortical or subcortical regions.

Third, the effects of neuromodulation were examined through pharmacological application of D1, D2, and M1 receptor agonists. While this approach isolated the contribution of each receptor type, it does not reflect the full complexity of in vivo neuromodulatory signaling, where multiple pathways act concurrently and interact with behavioral state.

Fourth, the MCFA method is linear in nature, the non-linear interactions between the attribute sets (AP, PB, AC, STA) are not captured by such a method. Therefore, the conclusion about co-regulations, especially in case of neuromodulation, should be interpreted with caution.  

Finally, in the computational modeling work, the performance of Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs) was benchmarked on a limited set of tasks. The generalizability of the observed heterogeneity and performance relationship to more complex architectures such as deep recurrent models, spiking neural networks, or biologically detailed simulations remains an open question.

While these limitations shape the scope of this study, they do not undermine its central contributions. The results related to heterogeneity remain robust, and the identified limitations instead point to concrete directions for further investigation in the direction of neuronal functional heterogeneity and networks. 

\subsection{Future Directions}

\subsubsection{Biological Extensions}

Future research should aim to extend electrophysiological recordings beyond step current protocols and further embrace dynamic, naturalistic stimulation regimes. One such recording protocol is the Dynamic Clamp protocol (\cite{robinson1993injection,sharp1993dynamic}). This would better approximate the range of inputs neurons experience in vivo and provide a richer substrate for functional classification. Additionally, integrating electrophysiology with other modalities such as transcriptomic profiling, morphological reconstruction, or in vivo imaging could yield a more comprehensive, multi-modal map of neuronal heterogeneity.

Another important direction is to link the observed functional reconfigurations induced by neuromodulation to behavior. There has been extensive body of work in this direction (\cite{Ott2023,Picciotto2012,Walters1987,Nadim2014, robinson2004firing}, which link the neuromodulation of single neurons by dopamine and acetylcholine to reward and learning. But a comprehensive high-dimensional view of single neurons' entire functional space (where each attribute is a dimension of this functional space) is missing. This potential area of research could involve combining neuromodulatory manipulation with behavioral paradigms and recording from populations in active animals, allowing us to trace how shifts in input filters or feature selectivity translate into changes in perception or motor output.

An important missing link in the context of our finding is the mechanistic explanation of heterogeneity found in linear input filters of the somatosensory neurons, a mechanistic understanding of how relatively homogeneous properties give rise to heterogeneous input filters is required. Also, it is an interesting direction to study the effect of input filter heterogeneity in networks. There has been a large body of work such as by (\cite{marder_multiple_2011,marder2014neuromodulation,kumari2024ion, edelman_degeneracy_2001,hennig_constraints_2018}) identifying degeneracy in ion-channel activation and high-level functional properties such as the linear input filter and in a similar way, degeneracy in the activity of single neurons to produce network states. I suggest to get a mechanistic understanding of how homogeneity in AP and PB parameters give rise to heterogeneous linear input filters. 

In \textbf{Chapter 3}, we found that sub-populations of neurons alter different sets properties due to neuromodulation, it is interesting to pursue this idea on a circuit level to understand how such heterogeneous modulation gives rise to attention in case of dopamine and relate it to behavior in general. This would require a large scale data collection and modeling effort. 

In this thesis I have identified several potential research directions to pursue, which will enhance neuronal taxonomy and the contemporary understanding of neuronal heterogeneity in general. This work will also further explain the effects of neuromodulation on a circuit level and lead towards mechanistic understanding of the effects of varying levels of heterogeneity among the four attribute sets.  

\subsubsection{Computational Extensions}
So far I expanded on the future directions to be pursued in the direction of neuronal heterogeneity with the perspective of neuromodulation and context. These are research directions grounded in empirical biology.  Besides the future research directions in neurobiology, there are many research directions to be pursued based on the computational part of this thesis. These research directions are aimed at synthesizing knowledge about neuronal heterogeneity and neuromodulation and use it towards designing biologically inspired neural networks.         

On the modeling side, it will be important to investigate whether the benefits of structured heterogeneity extend to more powerful or biologically plausible architectures, such as spiking neural networks, long short-term memory (LSTM) networks, or transformer-based recurrent systems. A related question is whether heterogeneity should be hand-crafted, as done here, or should be a part of the trained parameters. This could provide insight into how diversity emerges through development or adaptation. Evolutionary optimization or meta-learning approaches could be used to explore the space of possible heterogeneity structures. These tools may reveal principles for organizing diversity in artificial networks that generalize across tasks, paralleling the diversity we observe in biological circuits.

Another modeling choice to consider is the size of the reservoir itself. Our network size in the current experiment is small compared to a cortical circuits, a larger network would yield a clearer understanding of the effect of intrinsic heterogeneity on networks. Especially the effects of this heterogeneity of their their stability and memory. 

In summary, for our aim to understand the advantage of heterogeneity in neuronal computation at the level of cortical circuits, it is important to concern ourselves with biologically realistic networks such as spiking neural networks or cortical networks. Applying the observed heterogeneity in linear input filters as observed in \textbf{Chapter 2} and cell type specific neuromodulation on a network scale such as shown by \cite{liu2021cell} would provide a rich understanding of how these rather distant facets of heterogeneity (input selectivity and neuromodulation) come together to affect computation.    


\section{Final Conclusion}

In this thesis I show that neuronal functional identity is not static as commonly assumed but is rather dynamic depending on context and neuromodulation. I show that biologically realistic input is important for neuronal functional classification compared to a static step-and-hold input, moreover, neurons can be distinguished by their linear input filter. Taking this idea further, I show that neuromodulators act as a secondary controller of neuronal function, they change the coordination between functional attribute sets and also the transferred information in a cell-type specific manner. I also find sub-population of neurons with distinct modulation profiles. This motivates for a network level study that leverages this sub-population heterogeneity to understand what effect it has on the overall computational properties of networks. I find that reservoir computing networks benefit from subpopulation level heterogeneity. This showcases the importance of neural heterogeneity. 

\subsection{Epilogue}

It is clear to me that neuronal diversity is not noise, rather a fundamental feature of the brain's architecture. This heterogeneity shapes the structure and function of neural circuits, giving rise to the complexity of behavior, and experience. Therefore, understanding neural diversity is essential not just for neuroscience, but for comprehending the variability that defines human societies. A deeper grasp of this functional diversity is not optional; it is necessary for understanding ourselves and the systems we build.


\newpage
\begin{spacing}{1.0} % Set the line spacing to single spacing
\fontsize{8pt}{8pt}\selectfont
\bibliographystyle{apalike}
\renewcommand{\bibname}{References}
\bibliography{All_bibtex}
\end{spacing}