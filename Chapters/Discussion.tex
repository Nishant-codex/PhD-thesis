\fancyhf{}
\fancyhead[C]{Chapter 6. General Discussion}% <- added
\fancyfoot[R]{\thepage\ifodd\value{page}\else\hfill\fi}
%\fancyhead[L]{\ifodd\value{page}\relax\else\hfill\fi Ch \thechapter}
%\renewcommand\headrulewidth{0pt}% default ist .4pt
\renewcommand{\plainheadrulewidth}{.4pt}% default is 0pt


\newpage



\section{Summary}

\textbf{Chapter 2 }challenges the long-standing assumption in systems neuroscience that neuronal functional identity is fixed. Through direct comparison of neuronal responses to different input regimes, it demonstrates that functional identity is context-dependent, varying with the nature of the input. Moreover, it reevaluates the electrophysiological features commonly used in classification such as action potential dynamics and passive biophysical properties by showing that the linear input filter accounts for a greater proportion of variance across neurons. This makes it a more informative and discriminative feature for functional classification.

In \textbf{Chapter 3}, I show that neuromodulation alters information transfer in a cell-type and agonist-specific manner. These changes are not uniformly distributed across the neuronal population; instead, neuromodulation induces coordinated shifts in functional properties, often making subpopulations more similar in their modulation profiles. This adds further complexity to functional classification by showing that identity is not only input-dependent but also dynamically reshaped by internal network states.

Finally in \textbf{Chapter 4}, I extend the investigation to artificial systems, I examine how intrinsic timescale heterogeneity affects performance in reservoir computing networks. I demonstrate that Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs) exhibit improved task performance and memory capacity when composed of units with heterogeneous time constants. This supports the idea that structured diversity both in biological and artificial systems enhances computational capability.          


\section{Neuro-scientific Implications}
 
\subsection{Rethinking neuronal classification: input dependence challenges current taxonomies}

Understanding neuronal heterogeneity remains one of the most persistent challenges in systems neuroscience \cite{fishell2013neuron, huang2019diversity, markram2004interneurons, mukamel2019perspectives, nelson2006problem, petilla2008petilla, sanes2015types, seung2014neuronal, somogyi2005defined, yuste2020community, zeng2022cell, zeng2017neuronal}. This thesis challenges a foundational assumption: that neuronal functional identity is fixed and can be categorized independently of context. Despite large-scale classification efforts such as those by the Allen Brain Institute \cite{gouwens2019classification, gouwens2020integrated} the dependence of electrophysiological types (e-types) on the nature of synaptic input remains largely overlooked. Here, I provide evidence that neuronal classification is input-dependent, and that functional identity shifts under different stimulus regimes. This insight calls for a critical reassessment of current classification schemes in systems neuroscience.

The widely used step current protocol has been instrumental in uncovering intrinsic properties such as action potential dynamics \cite{halabisky2006electrophysiological,jasnow2009distinct,karagiannis2009classification}, passive membrane features \cite{hernath2019alternative,szabo2021conventional}, and excitability types \cite{connors1990intrinsic,hodgkin1948local,izhikevich2007dynamical}. However, it is fundamentally limited: neurons in vivo rarely receive stationary inputs \cite{zeldenrust2017estimating}. Instead, they are embedded in constantly fluctuating networks, driven by dynamic, temporally structured synaptic activity. Probing neurons with dynamic inputs, such as frozen noise \cite{zeldenrust2017estimating}, better captures their functional operating range and reveals aspects of computation that static protocols obscure.

These findings advocate for a paradigm shift: functional classification must be grounded in how neurons respond to realistic, time-varying inputs. Achieving this requires a renewed effort in data collection one that prioritizes dynamic stimulation paradigms. While more resource-intensive, this approach offers a deeper, more ecologically valid understanding of neuronal function, and will ultimately yield more meaningful classifications for modeling brain computation.

\subsection{Introduction of Novel Analytical Methods to Study Functional Reorganization and High-Dimensional Correlations}

Addressing the dynamic nature of neuronal functional classification and heterogeneity required the application of analytical tools not traditionally used in electrophysiological studies. To this end, I employed unsupervised machine learning techniques specifically, Louvain clustering combined with UMAP for dimensionality reduction, and Multi-set Correlation and Factor Analysis (MCFA) \cite{brown2023multiset} for feature-space alignment and integration.

While Louvain+UMAP has been previously applied to waveform clustering \cite{lee2021non}, its application to electrophysiological feature spaces across distinct input contexts is novel. In this thesis, I leveraged this framework to directly compare neuronal classifications under different stimulation regimes (e.g., step current vs. frozen noise), revealing context-dependent shifts in functional identity.

A persistent challenge in electrophysiological classification is the lack of consensus on which features most effectively capture neuronal heterogeneity. To move beyond arbitrary or limited feature selection, I employed MCFA to quantify the correlation structure across multiple high-dimensional property sets such as action potential dynamics, adaptation currents, passive biophysics, and spike-triggered averages. This allowed us to identify which sets are most informative for characterizing functional diversity. Furthermore, we used MCFA to assess how these correlation structures change under neuromodulatory influence, offering new insight into how functional properties reorganize in response to network state changes.

Although UMAP+Louvain and MCFA are established tools in genomics and single-cell omics \cite{brown2023multiset}, their adoption in systems neuroscience remains limited. This thesis demonstrates their applicability to electrophysiological data and advocates for their broader use in analyzing the high-dimensional functional landscape of neurons. With more comprehensive datasets, these methods are likely to yield deeper, more robust insights into functional heterogeneity and neuronal classification.


\subsection{Highlighting the Functional Value of Heterogeneity at the Population Level}

In Chapter 2, I examined heterogeneity across four electrophysiological feature sets: action potential (AP) dynamics, passive biophysical (PB) properties, adaptation currents (AC), and spike-triggered averages (STA). Among these, STA emerged as uniquely informative for capturing population-level heterogeneity. \textbf{This indicates that even within a single cortical region the somatosensory cortex neurons differ substantially in their linear input filters.} Such diversity implies a level of functional specialization, where neurons are tuned to distinct input features. This specialization may provide a basis for modularity in cortical computation and aids cognitive flexibility \cite{wu2025neural,hutt2023intrinsic}. It is likely that this heterogeneity would be even more pronounced when comparing neurons across different cortical areas. These findings argue that cortical models must incorporate variability in feature selectivity to accurately reflect population-level dynamics.

In Chapter 3, I extended this analysis to neuromodulatory effects, focusing on dopaminergic (D1, D2) and cholinergic (muscarinic M1) receptor activation. Neuromodulation altered both the amount of information transmitted and the classification of neurons across all four feature sets. In addition, it reshaped the correlation structure among these features, demonstrating that neuromodulation \textbf{does not act on single properties in isolation but reorganizes the broader functional landscape of neuronal populations}. Notably, these effects were heterogeneous: subpopulations of neurons responded differently to each receptor agonist, indicating that neuromodulators can act as fine-tuned control mechanisms that selectively modulate specific subsets of the circuit \cite{ogawa2023multitasking,salvan2023serotonin,gast2024neural}. \textbf{This suggests a dual role for neuromodulation both amplifying baseline heterogeneity and exploiting it to regulate population-level dynamics in a context-sensitive manner.}

\section{AI / computational modeling implications}


\subsection{Heterogeneity Improves Reservoir Performance}
In Chapter 4, I demonstrate that timescale heterogeneity enhances the performance of both Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs), particularly in terms of memory capacity and task accuracy. However, the relationship is not linear: the most heterogeneous architecture the heterogeneous cluster does not always yield the best results. \textbf{This underscores a key insight: the benefits of timescale heterogeneity are task-dependent, and excessive heterogeneity can, in some cases, degrade performance.}

Notably, DDNs already incorporate heterogeneous inter-unit delays by design, which contributes to a more distributed memory profile compared to ESNs. This structural feature gives DDNs a baseline advantage in tasks requiring temporal integration. Adding intrinsic decay heterogeneity further enhances performance as shown in the memory capacity analysis but only up to a point. Beyond a certain level of heterogeneity, the gains diminish or reverse.

The benchmark tasks reinforce this tradeoff. In the NARMA-30 task, which demands precise short-term memory, moderate timescale heterogeneity improved performance in both ESNs and DDNs. In contrast, for the Mackey-Glass task a chaotic time-series prediction problem excessive heterogeneity introduced instability, reducing predictive accuracy. These results highlight the need to calibrate heterogeneity to the specific computational demands of the task.

Interestingly, both homogeneous cluster and heterogeneous cluster architectures outperformed fully homogeneous or fully heterogeneous networks across tasks. This suggests that structured heterogeneity variation organized into modular subgroups offers an optimal balance between flexibility and control.\textbf{ These findings emphasize that while heterogeneity is beneficial, its impact depends critically on its organization within the network.}



\subsection{Toward Architecture-Level Design Principles Grounded in Biology}

\textbf{Chapters 2 and 3} reveal a fundamental insight into the extent and structure of heterogeneity in the somatosensory cortex. By extracting multiple facets of neuronal function including action potential dynamics, adaptation currents, and input selectivity I demonstrate that neurons within a single cortical region vary widely in how they process stimuli. This functional diversity contributes to the brain’s adaptability in responding to dynamic and unpredictable inputs.

In \textbf{Chapter 4}, I abstracted this biological heterogeneity into artificial systems by implementing it within reservoir computing architectures. The results confirmed a core hypothesis: \textbf{heterogeneity at the unit level enhances the computational performance of interconnected systems}. These findings demonstrate that architectural principles rooted in observed biological phenomena can lead to more effective and flexible artificial systems.

Moreover, the findings point to a second design insight: heterogeneity must be \textbf{task-aware}. As shown in the benchmark experiments, increasing diversity blindly can be counterproductive. Instead, architectural decisions should reflect both biological principles and task-specific constraints. In this context, “biologically inspired” design must not mean mimicking biology uncritically, but rather \textbf{translating its organizing logic} into systems that serve defined functional goals.

\subsection{Limitations and Methodological Constraints}

Despite the strengths of this thesis, several limitations should be acknowledged. First, some electrophysiological recordings were constrained by the size of the dataset and the experimental protocol. In particular, while frozen noise stimulation provided a richer input space than step protocols, the overall number of cells recorded under each condition limited the statistical power for detecting more subtle subpopulation-level effects. This also applies to the recordings for each agonist condition. 

Second, while the classification results revealed clear context- and input-dependent changes in neuronal identity, these findings are currently restricted to neurons within the somatosensory cortex. It remains to be seen whether similar dynamic reconfigurations occur in other cortical or subcortical regions.

Third, the effects of neuromodulation were examined through pharmacological application of D1, D2, and M1 receptor agonists. While this approach isolated the contribution of each receptor type, it does not reflect the full complexity of in vivo neuromodulatory signaling, where multiple pathways act concurrently and interact with behavioral state.

Fourth, the MCFA method is linear in nature, the non-linear interactions between the attribute sets are not captured by such a method. Therefore, the conclusion about co-regulations especially in case of neuromodulation should be interpreted with caution.  

Finally, in the computational modeling work, the performance of Echo State Networks (ESNs) and Distance-Dependent Delay Networks (DDNs) was benchmarked on a limited set of tasks. The generalizability of the observed heterogeneity-performance relationship to more complex architectures such as deep recurrent models, spiking neural networks, or biologically detailed simulations remains an open question.

\subsection{Future Directions}

\subsubsection{Biological Extensions}

Future research should aim to extend electrophysiological recordings beyond step current protocols and further embrace dynamic, naturalistic stimulation regimes. These would better approximate the range of inputs neurons experience in vivo and provide a richer substrate for functional classification. Additionally, integrating electrophysiology with other modalities such as transcriptomic profiling, morphological reconstruction, or in vivo imaging could yield a more comprehensive, multi-modal map of neuronal heterogeneity.

Another important direction is to link the observed functional reconfigurations induced by neuromodulation to behavior. This could involve combining neuromodulatory manipulation with behavioral paradigms and recording from populations in active animals, allowing us to trace how shifts in input filters or feature selectivity translate into changes in perception or motor output.

An important missing link in the context of our finding is the mechanistic explanation of the observed phenomenon, a mechanistic understanding of how relatively homogeneous PB and AP properties give rise to heterogeneous input filters is required. Also, it is an interesting direction to study the effect of input filter heterogeneity in networks.

In Chapter 3 we found the emergence of subpopulations with heterogeneous modulation, it is interesting to pursue this idea on a circuit level to understand how such heterogeneous modulation dives rise to attention in case of dopamine and relate it to behavior in general. This would require a large scale data collection and modeling effort. 

\subsubsection{Computational Extensions}

On the modeling side, it will be important to investigate whether the benefits of structured heterogeneity extend to more powerful or biologically plausible architectures, such as spiking neural networks, long short-term memory (LSTM) networks, or transformer-based recurrent systems. A related question is whether heterogeneity should be hand-crafted, as done here, or learned through training a comparison that could provide insight into how diversity emerges through development or adaptation. 

Our network size in the current experiment is small compared to a cortical circuits, in order to have a clearer understanding of the effect of heterogeneity on networks, we need to study bigger networks and therefore its stability and memory distribution. 

Evolutionary optimization or meta-learning approaches could be used to explore the space of possible heterogeneity structures. These tools may reveal principles for organizing diversity in artificial networks that generalize across tasks, paralleling the diversity we observe in biological circuits.

Finally, for our aim to understand biological underpinnings of heterogeneity in neuronal computation it is important to concern ourselves with biologically realistic networks such as spiking neural networks or cortical networks. Applying the observed heterogeneity in linear input filters and modularity in sub-populations on a network scale such as shown by \cite{liu2021cell} would provide much richer understanding of how these rather orthogonal facets of heterogeneity come together to affect computation.    

\section{Final Statement}

My aim with this thesis was to explore neuronal heterogeneity using single unit electrophysiological recordings from the somatosensory cortex. I succeeded in putting forth electrical diversity in a specialized cortical region not just in terms of legacy features but by looking the richer high dimensional feature space, I also explored the effect of neuromodulation on this baseline heterogeneity and finally used the abstracted observations on a reservoir system to study the effect on memory and processing in the network.    

It is clear to me that neuronal diversity is not noise, rather a fundamental feature of the brain's architecture. This heterogeneity extends beyond individual neurons to shape the structure and function of neural circuits, giving rise to the complexity of thought, behavior, and experience. Because brains construct the foundation of individual cognition and collective culture, we can argue that this diversity is what makes our world richer with beauty and takes us forward. Therefore, understanding neural diversity is essential not just for neuroscience, but for comprehending the variability that defines human societies.  A deeper grasp of this functional diversity is not optional; it is necessary for understanding ourselves and the systems we build.


\newpage
\begin{spacing}{1.0} % Set the line spacing to single spacing
\fontsize{8pt}{8pt}\selectfont
\bibliographystyle{apalike}
\renewcommand{\bibname}{References}
\bibliography{All_bibtex}
\end{spacing}