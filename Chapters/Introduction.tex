\clearpage
\fancyhf{}
\fancyhead[c]{Chapter 1. General Introduction}% <- added
\fancyfoot[R]{\thepage\ifodd\value{page}\else\hfill\fi}
%\fancyhead[L]{\ifodd\value{page}\relax\else\hfill\fi Ch \thechapter}
%\renewcommand\headrulewidth{0pt}% default ist .4pt
\renewcommand{\plainheadrulewidth}{.4pt}% default is 0pt

\newpage

\noindent I urge you to look around you and observe the trees, the leaves on them, each one with a unique shape and size, isn't it? Look at the people surrounding you, how they have different skin colors, facial features, heights, voice and thoughts. Now take a look at your palm, go ahead and examine it. What do you see? Aren't all five of your fingers different from each other? If you look even closer, you'd see that even the scales and crevices on your fingers are not the same. Have you ever wondered why nothing is exactly the same? Variance is an essential feature of nature; it permeates everything, from our thoughts and dreams to our language and societies. Without this diversity, life would be sterile, perhaps even impossible.

In this thesis, I have sought to understand the origins and implications of these differences at the most 
fundamental level: the neurons inside our brains. By exploring the variability among these building blocks 
of thought, ideas, dreams, language, and personality, I aim to shed light on how diversity shapes not only 
our individual experiences but also the very fabric of life. Neuroscience has long treated neurons as discrete, static ‘cell types’. But accumulating evidence shows identity lies on a continuum, is context-dependent, and heterogeneous. My aim with this thesis is to demonstrate that neuronal identity, especially the functional identity is not static but is rather dependent on the context and level of receptor specific neuromodualtion.  


\section{Neuronal heterogeneity through the lens of the past}

The mammalian brain is an extraordinarily complex organ, composed of an immense variety of neuronal cell types. This diversity is evident across multiple dimensions. Exploring this diversity has been a central tenet in neuroscience, since its inception, dating back to Ramon y Cajal (\cite{fishell2013neuron, huang2019diversity, markram2004interneurons, mukamel2019perspectives, nelson2006problem, petilla2008petilla, sanes2015types, seung2014neuronal, somogyi2005defined, yuste2020community, zeng2022cell, zeng2017neuronal}). I would like to take the reader through what we already know about neuronal heterogeneity via different modalities, 
ranging from shapes to molecular composition.    

\subsection{Morphology}

Neurons differ dramatically in their shapes and structures. Some, like pyramidal neurons, exhibit long 
apical dendrites and a characteristic triangular soma, while others, such as inter-neurons, display 
more compact and intricate branching patterns. These morphological differences are closely linked to 
the specific roles neurons play within neural circuits (\cite{liu2024neuronal}).


\subsection{Electrophysiology}

Neuronal diversity is also exhibited in the ways neurons generate and propagate electrical signals. 
Neurons exhibit distinct firing patterns, action potential shapes, spike frequency adaptation and selectivity to synaptic input. For instance, some neurons are fast-spiking, while others display spike frequency adaptation or bursting firing patterns. These electrophysiological properties are determined by the unique composition of ion channels and receptors expressed by each neuron (\cite{huang2019diversity, markram2004interneurons, mukamel2019perspectives, fishell2013neuron, masland2012neuronal, tasic2018shared, zeng2017neuronal}).


\subsection{Gene Expression}

Advances in molecular biology have revealed that neurons can be distinguished by their gene expression profiles. Single-cell RNA sequencing has enabled researchers to catalog the transcriptomes of individual neurons, uncovering a rich landscape of molecular identities. These profiles often correlate with, but are not strictly determined by, morphological and electrophysiological features (\cite{wagner2016revealing, shapiro2013single, trapnell2015defining}).


\subsection{Connectivity}

Neurons are further defined by their patterns of connectivity, both the sources of their inputs and the targets of their outputs. Some neurons form long-range projections across brain regions, while others 
participate in local microcircuits. This connectivity further defines the broader neural circuit function. (\cite{Wang2021, Zhang2024, Gollo2020, Goldman2023, Tripathy2017, Sagner2019} )

These modalities highlight the fact that neuronal heterogeneity is multi-faceted, and challenges the notion of n-neuron types. There have been many integrative efforts that merge these different modalities. 

\subsection{Cataloging neuronal heterogeneity}

In the following section we highlight the large scale efforts to study neuronal heterogeneity based on different modalities as well as the efforts towards integrating these individual modalities into a complete taxonomic picture of neuronal heterogeneity in the brain. 

\subsection{Large-Scale Taxonomy Initiatives}

In the past decade, large-scale collaborative efforts have sought to systematically map and classify the full 
diversity of neurons in the brain. Notable among these are the Allen Institute for Brain Science's Cell Types 
Program and the BRAIN Initiative Cell Census Network (BICCN). These projects leverage cutting-edge techniques, including high-throughput single-cell transcriptomics, large-scale electrophysiological recordings, and high-resolution imaging to build comprehensive taxonomies of neuronal types \cite{gouwens2019classification,gouwens2020integrated}.

While these initiatives have greatly expanded our understanding of neuronal diversity, they often operate under the assumption that neuronal identity is static and can be captured by a fixed set of features. However, emerging evidence suggests that neuronal identity may be more dynamic and context-dependent than previously thought, raising important questions about how best to define and classify the brain's myriad cell types.

\subsection{Patch-seq}

The state of the art in recording neuronal data is a technique known as patch-seq (\cite{cadwell2016electrophysiological, fuzik2016integration}), whereby it is possible to simultaneously record morphological, electrophysiological and molecular properties of neurons. This technique is really powerful as it provides a multi-modal perspective on neuronal heterogeneity. This technique has been the linchpin in some of the biggest classification efforts such as by the Allen Brain database (\cite{gouwens2019classification, gouwens2020integrated}), with a multi-modal collection of more than 1900 neurons. 

\section{Classification }

So far we have established that neuronal heterogeneity is multi-modal and the amount of data is growing exponentially. In order to leverage this growth in data and multi-modal picture, it becomes apparent to categorize neurons based on their similar characteristics. In the following sections we will look at some of the classification approaches and discuss their limitations.  

\subsection{Traditional Classification Approaches}

Historically, neuroscientists have sought to classify neurons into discrete “types” based on intrinsic, 
relatively stable characteristics. Early classification schemes focused on observable features such as 
soma size, dendritic arborization, and axonal projections. With the advent of intracellular recording techniques, electrophysiological properties like spike shape, firing rate, and synaptic integration became central to neuronal taxonomy. More recently, molecular markers such as the expression of specific neurotransmitters, calcium-binding proteins, or transcription factors have been used to further refine neuronal classifications.

The underlying assumption in many of these approaches is that each neuron possesses a fixed identity: a stable set of features that persists across time and context. This has led to the widespread use of terms like 
''cell type'' and ''canonical neuron'', suggesting a degree of invariance in neuronal identity.

Here I challenge this classic view by testing if neuronal classification depends on the external factors such the somatic input distribution and neuromodulation. I have identified several limitations to this assumptions which involve the the way neurons interact with the network in which they are embedded and the recordings protocols. In the next section I am going to highlight the most pressing limitations which establish that the idea of static neuronal identity needs a revision.  


\subsection{Limitations of Static Neuronal Classification}

In this section we discuss some of the limitations of a static neuronal classification. For a long time neuronal function has been discussed in the field of neuroscience as invariant. This is apparent with the input protocol used to study function of neurons. For example, since the beginning of single unit patch-clamp recordings, a static step input has been used to probe the firing rates of neurons. The following points challenge this view by highlighting problems with this assumption: 

\begin{itemize}

    \item  \textbf{The Influence of Input and Network State on Neuronal Function} Recent experimental and computational efforts have increasingly pointed towards the idea that a neuron's functional role is not solely determined by its intrinsic static properties such as morphology, gene expression, or ion channel 
dynamics but also by the nature of the input it receives and its dynamic state within a circuit (\cite{hernath2019alternative,szabo2021conventional}). Neurons operate within continuously changing environments, receiving temporally structured synaptic input that reflects sensory stimuli, behavioral 
demands, and ongoing internal activity. These inputs interact with intrinsic biophysical parameters in 
complex ways, such that the same neuron may perform different computational roles depending on the input regime or network context. Additionally, neuromodulatory systems (e.g., dopaminergic and cholinergic pathways) further reconfigure neuronal function in a cell-type- and receptor-specific manner, modulating excitability, gain, adaptation, and stimulus selectivity. This context-dependence challenges the classical view of neurons as fixed computational units. 
    
    \item \textbf{Continuous neuronal identity}
Recent large scale neuronal classification efforts have consistently shown that neurons are not discrete 
modular classes but rather that their electrophysiological, molecular and morphological properties lie on a 
continuum \cite{marsat2010neural,angelo2012biophysical,scala2021phenotypic}. This further complicates the effort of classifying neurons in classes. 

    \item \textbf{Limitations of Static Characterization Protocols} Conventional approaches to neuronal classification typically rely on static stimulation protocols, such as step-and-hold current injections, to derive electrophysiological signatures. While these protocols offer insights into baseline excitability and passive membrane properties, they do not capture the complex temporal filtering or nonlinear input-output transformations that neurons perform under more realistic, time-varying conditions. In naturalistic settings, 
neuronal input is dynamic, high-dimensional, and often stochastic features that are absent in traditional 
measurements. As a result, static protocols risk underestimating or mis-characterizing the functional capabilities of neurons, potentially leading to oversimplified or misleading classifications. This disconnect 
highlights the need for rich stimulus paradigms, such as temporally varying current or (dynamic clamp) conductance stimuli or in vivo recordings, that better approximate the computational demands faced by neurons in situ.

    \item \textbf{Neuronal Identity as an Emergent, Dynamic Construct} Taken together, these observations point 
toward a new conceptual framework in which neuronal identity is not a fixed property, but rather a 
context-sensitive, emergent phenomenon. If neuronal function can be reshaped by synaptic input, neuromodulatory state, and network dynamics, then neuronal identity must be understood as fluid and multidimensional, rather than static and categorical. This dynamic view aligns with recent findings showing that neurons shift their classification depending on the input stimulus or modulatory condition. It also suggests that functional heterogeneity in neural populations is not simply biological variability, but may reflect adaptive specialization to a range of computational roles. 

In this thesis, I explore this emergent view by analyzing how neurons reorganize their functional attributes 
under different input regimes and neuromodulatory conditions, using high-dimensional clustering and 
integrative analysis across multiple feature domains. This work contributes to a growing shift in neuroscience: from static taxonomies to dynamic, functionally grounded models of neuronal identity. 
    
    \item \textbf{Rich high-dimensional functional space remains unexplored} The functional space of neurons is high 
dimensional, this is due to the fact activity of neurons depends on the activation/inactivation of ion multiple ion-channels (\cite{fyon2024dimensionality}), this can be summarized by passive properties such as resistance, capacitance, etc. Furthermore, neurons can filter inputs in temporal and spacial dimensions which further adds to the dimensionality (\cite{y2003computation}). But most electrophysiological classifications only consider one feature at a time. This leads to an incomplete picture of functional heterogeneity.  As we have discussed before, stimulus protocols have a strong role to play in features that can actually be extracted. For example a static step and hold input protocol doesn't expose the subthreshold potential dynamics of a neuron. Similarly, properties related to action potential are a function of the input protocol. Clustering based on these features typically involves looking at a low dimensional (1-3 dimensions) feature space using a method such as K-means clustering. This approach underestimates the rich functional space in which neurons function and thus clustering based on functional properties needs a method that utilizes multiple features pertaining to function simultaneously and provides a rich overview of functional heterogeneity. 
    
    \item   \textbf{No consensus on properties that are the most informative about heterogeneity} While neurons have been categorized based active and passive features there is no consensus on properties or a set of properties that are the most informative about heterogeneity. While most classification studies focus on passive properties such as capacitance or conductance of the cell or active properties such as firing rates and inter-spike intervals, there are no studies that compare the estimate of neuronal heterogeneity based on different sets of properties and provide a consensus for the field. Moreover, the properties extracted are limited by the stimulation protocol. For example it is not possible to estimate the linear input-filter of a neuron via a step input protocol, and thus it requires a white noise based stimulation protocol. Therefore, we must first establish the input protocol that is most suitable for studying a neuronal population and then compare properties extracted in each protocol to provide a consensus.  
\end{itemize}

The limitations listed above show that assuming a static neuronal identity is restrictive, it gives an incomplete picture of a rich high-dimensional and dynamic functional space in which neurons reside. Therefore a new method is required to ascertain the high dimensional and dynamic nature of neuronal function by taking into account the input context and functional properties used to study function. 

So far we have highlighted the limitations of input protocol and the lack of consensus in determining the most informative properties to study functional heterogeneity, but a very important still remains to be discussed and that is neuromodulation. Neurons consist of ion-channels that are susceptible to modulation via specific receptor activation molecules. These neuromodulators have a strong effect on the activity of neurons, they can suppress or enhance firing of a neuron. Therefore it is important to study how neuromodulation affects neuronal function and classification. The following section discusses the current state of understanding in terms of neuromodulation.   

\section{Neuromodulation: Reconfiguring Neural Computation}

Neuromodulatory systems, particularly those involving dopamine and acetylcholine play a critical role in shaping brain states associated with attention, learning, memory, and pathology. Rather than directly triggering spikes, these modulators reshape how neurons respond to input, modulating intrinsic excitability and synaptic integration. Their effects are often receptor and cell-type-specific, suggesting a finely tuned mechanism for reconfiguring the computational landscape of cortical networks.

Despite extensive behavioral and molecular work (\cite{nadim2014neuromodulation,mccormick2020neuromodulation,avery2017neuromodulatory}), the functional impact of neuromodulation on neuronal computation remains poorly understood, especially at the level of high-dimensional feature interactions. Are neuromodulatory effects isolated to individual electrophysiological features, or do they act in a coordinated manner to restructure how neurons encode information?

The following points summarize the gaps of what remains to be understood in regards to the effect of neuromodulation, specifically Dopaminergic and Cholinergic modulation, on single neurons: 

\begin{itemize}
    \item \textbf{Effect of neuromodulation on computation} Much is known \cite{dalley2004cortical,hasselmo2006cholinergic,sarter2009phasic,nusser2009variability} about molecular and excitability changes caused by dopamine and acetylcholine modulation in single neurons. Though it is still unclear how this alteration in excitability changes what neurons compute. Since neurons relay information within a network, it is unclear how dopamine and acetylcholine changes information transfer in neurons. Since activity of individual neurons determine that state of a network, it is clear that understanding alteration in transferred information due to neuromodulation would explain a lot about how cortical circuits are modulated. 

    \item \textbf{Heterogeneous modulation in neuronal population} A given population of neuron comprises of individuals or sub-groups of neurons that differ from each other in terms of their shape and and ion-channel type and distribution. It is still unclear how this individual variability manifests itself on a circuit scale. While it is known that this heterogeneity provides flexibility to  the circuit (\cite{wu2025neural,hutt2023intrinsic}), the effect of neuromodulation on this individuality is still unclear. More precisely, it is unknown if all functional properties are altered uniformly or if there exists a heterogeneity in terms of properties that are altered due to specific modulation and what are population level effects of neuromodulation. 

    \item \textbf{Reconfiguration under the influence of dopamine and acetylcholine} Most studies that focus on neuromodulation of individual neurons, focus on how individual active or passive properties such as firing rate or conductance are altered due to a specific receptor activation. While it is important to study how individual properties are altered due to neuromodulation, it is also important to study how the high-dimensional functional space of neurons is altered due to neuromodulation. This requires looking at correlation structure between active and passive properties of neurons and how this correlation structure changes due to neuromodulation.
\end{itemize}

So far we have looked at established view on neuronal heterogeneity, limitations of static neuronal classification, the effect of neuromodualtion on reconfiguring neuronal computation. Although much is known about heterogeneity in neural systems, it is still not completely understood how this intrinsic heterogeneity affect network function. In this next section we set the context for studying heterogeneity in an artificial network.  
          
\section{Effect of Temporal Heterogeneity in Artificial Systems}

Biological neural systems are composed of heterogeneous computational units (individual neurons) with diverse intrinsic properties (\cite{koch1999complexity}). This diversity has been shown to support critical functions such as motor control (\cite{cavanagh2020diversity}) and memory formation (\cite{mcnaughton2006path, hasson2008hierarchical, chu2020long}). While this fact is well-accepted in neuroscience, most artificial neural networks continue to be designed with homogeneous units.

Recurrent Neural Networks (RNNs), which possess temporal memory, have been proposed as functional analogs to biological circuits (\cite{sussillo2014neural}). One popular subclass, Reservoir Computing (RC) networks, or Echo State Networks (ESNs), have been demonstrated to have the capacity to solve tasks requiring short-term memory (\cite{jaeger2001echo}). However, these networks are typically built with uniform time constants and lack the diversity observed in biological systems.

Recent studies have introduced architectural temporal heterogeneity through mechanisms like delay lines. In particular, Distance-based Delay Networks (DDNs) have been shown to outperform classical ESNs on memory and chaotic time-series tasks (\cite{iacob2022distance, soriano2014delay}). Previous studies have also shown benefits of intrinsic heterogeneity in spiking network, more specifically on encoding and memory task (\cite{gast2024neural,perez-nieves_neural_2021,chakraborty_heterogeneous_2022,zeldenrust_efficient_2021, mejias_optimal_2012,tripathy_intermediate_2013,landau_impact_2016,doty_heterogeneous_2021,duarte_leveraging_2019}). These results suggest that incorporating temporal diversity improves computational performance. However, in such models, heterogeneity is embedded in network-level topology, not in the intrinsic properties of individual units.

Thus, it remains unclear how intrinsic temporal heterogeneity such as variability in decay rates of individual units in the network or integration time constants across individual units influences network performance. This thesis explores this gap by systematically varying internal timescales within reservoir units, testing whether biologically inspired temporal diversity can enhance memory capacity and task performance in artificial networks.

\section{Research Objectives and Key Questions}
This thesis addresses three overarching questions:

\begin{enumerate}
  \item \textbf{How does input context influence functional classification of neurons?} \\
In Chapter 2, we demonstrate that neuronal identity is not static, but input-dependent, with classification outcomes varying significantly under dynamic (frozen noise) vs static (step-and-hold) stimuli. This challenges the notion of fixed electrophysiological types and highlights the role of stimulus structure in shaping neural function.

  \item \textbf{How does neuromodulation alter the functional landscape of neurons?} \\
In Chapter 3, we explore how activation of dopaminergic (D1-R, D2-R) and cholinergic (M1-R) receptors reconfigures information encoding, shifting the correlation stricture between feature domains. We show that neuromodulatory effects are highly cell-type and receptor-specific, modulating not just individual functional attributes but also their coordination.

  \item \textbf{Can intrinsic timescale heterogeneity improve performance and memory in reservoir computing systems?} \\
In Chapter 4, we go from data analysis to network level study of intrinsic heterogeneity, for this we design 4 different variants of ESNs and DDNs with varying levels of timescale heterogeneity and optimize them to perform NARMA-30 and Mackey-Glass benchmark tasks. We study the effect of the designed heterogeneity on task performance, stability, representation and memory capacity. We show that an intermediate level of heterogeneity is optimum for solving the two benchmark tasks, suggesting heterogeneity requirement is task dependent.     
  
\end{enumerate}


\section{Experimental Framework and Approach}

For answering the first two research questions stated above, this thesis utilizes single unit in-vitro recordings form the somatosensory cortex layer 2/3 (\cite{da2018databank}). These recordings were performed in tandem, first using a static step and hold input protocol followed by a frozen noise protocol (see Fig. \ref{fig:input_protocol}). Within each input protocol, neurons were first recorded in a vehicle control (artificial cerebral spinal fluid - aCSF) condition and a drug condition, where a specific receptor agonist is added to the bath and the recording is performed again. Sometimes multiple control and drug trials are recorded for a single neuron.   

\subsection{Input Protocols }
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Intro/input_protocol.pdf}
    \caption{\textbf{Data collection and input protocol} the top figure shows the process of input generation using the hidden state and an artificial neural network. The middle figure shows the recording process, the neurons are recorded first in control condition using the frozen noise and step and hold inputs respectively, and then a specific receptor agonist is added, and the recording is repeated again. The bottom figure shows the procedure for measuring transferred information. Mutual information is measured between the hidden state and the input as well as between the hidden state and the spike train. } 
    \label{fig:input_protocol}
\end{figure}

\subsubsection{Step and Hold}

The step-and-hold protocol is a widely used electrophysiological method for characterizing neuronal properties through controlled current injections. In this protocol, a neuron's membrane potential is maintained at a baseline value (commonly around -70 mV), and then a series of incremental current steps are injected, each lasting for a fixed duration, typically 500 ms with recovery periods (e.g., 5.5 seconds) between steps. The current amplitude is increased in defined increments (e.g., 40 pA per step), allowing researchers to observe how the neuron responds to increasing levels of depolarizing input, including changes in firing rate, spike threshold, and other action potential characteristics. This approach enables the classification of neurons (such as distinguishing excitatory from inhibitory cells) and the assessment of properties like maximum firing frequency, spike latency, and after-hyperpolarization. The step-and-hold protocol thus provides a standardized way to probe intrinsic excitability and firing dynamics, serving as a foundational tool in cellular neurophysiology. 

\subsubsection{Frozen Noise}

The frozen noise protocol (\cite{zeldenrust2017estimating}), is a method designed to quantify the mutual information between a neuron's input and its spike train output in electrophysiological experiments. This protocol generates a time-varying input current by simulating the activity of a presynaptic neural network of 1,000 neurons, each firing Poisson spike trains in response to a binary "hidden state" (a Markov process representing the presence or absence of an external stimulus). 
The injected current is "frozen", meaning the same input sequence is used across trials or conditions, enabling a direct comparison of neuronal responses. By analyzing how the recorded neuron transforms this structured input into spikes, researchers can calculate the information-theoretic relationship between the hidden state and the output spike train. 
This approach overcomes limitations of traditional step-and-hold protocols by mimicking naturalistic synaptic input patterns while maintaining experimental control, allowing efficient bias-free information quantification with short (6 minutes) recordings. The protocol's output includes the injected current trace, hidden state time series, and the neuron's voltage response, facilitating both forward modeling of neuronal dynamics and reverse-engineering of coding principles. 

\subsection{Extracted features}
In order to provide a consensus about attributes sets that captures the highest amount of functional heterogeneity, we extracted neuronal function attributes categorized into four distinct attribute sets. The grouping is performed in order to capture distinct facets of neuronal function. This includes action potential shape and dynamics, passive properties associated with the physical aspect of neurons. The adaptation properties and the feature selectivity:

\begin{itemize}
  \item \textbf{Action Potential (AP) attributes} This set contains 22 features related to action potential shape and dynamics with their descriptive statics.  
  \item \textbf{Passive Biophysical (PB) attributes} This set contains 6 features related to passive properties such as membrane capacitance.   
  \item \textbf{Adaptation Currents (AC)} This attribute is a curve that captures the adaptation dynamic after an action potential.   
  \item \textbf{Input Feature Selectivity}, estimated using \textbf{Spike-Triggered Averages (STA)} This is an attribute that captures the input feature selectivity of neurons. 
\end{itemize}



An illustration of the feature extraction and analysis is shown in Fig. \ref{fig:feature_extraction}. To dissect the specificity and dynamics of neuronal encoding depending on input and neuromodulation, we apply unsupervised high-dimensional clustering (\cite{lee2021non}), cosine similarity analysis, and Multi-set Correlation and Factor Analysis (MCFA) (\cite{brown2023multiset}). This methods allow us to examine both within-attribute set variance and cross-attribute set coordination of attributes, offering a system-level view of functional reconfiguration.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Intro/analysis_tools.pdf}
    \caption{\textbf{Overview of extracted features and analysis tools}}
    \label{fig:feature_extraction}
\end{figure}

\subsection{Network Design}
In order to study the effect of heterogeneity of intrinsic properties, in this case the time constant of individual units on computational properties of networks, we designed four different heterogeneity types for ESNs and DDNs, these configuration capture different levels of heterogeneity, we tested network wide heterogeneity and contrasted it with the network divided into clusters, with each cluster having its own parameter sampled from a distinct distribution. The designed network configurations are summarized as following: 
\begin{enumerate}
    \item \textbf{Homogeneous Network}: All units share a single decay (time constant) parameter.
    \item \textbf{Homogeneous Cluster}: The ESN/DDN is divided into clusters, each cluster with its own fixed decay parameter.
    \item \textbf{Heterogeneous Network}: Each unit samples its decay from a shared distribution.
    \item \textbf{Heterogeneous Cluster}: Each cluster samples decay parameters from different distributions.
\end{enumerate}

We also wanted to test if network heterogeneity is task dependent, which means if task complexity decides the requirement for the optimum level of network heterogeneity. We tested these networks on two benchmark tasks namely NARMA-30 and Mackey-Glass tasks:
\begin{itemize}
    \item \textbf{NARMA-30}: A nonlinear auto-regressive moving average task designed to test long-range memory and nonlinear dynamics. 
    \item \textbf{Mackey-Glass}: A chaotic time-series prediction task that evaluates a network's ability to generate stable yet complex temporal outputs.
\end{itemize} 

We aimed to train these designed ESNs and DDNs on the two tasks mentioned above and study the effect of intrinsic heterogeneity on stability, memory capacity and dimensionality of these networks. 

\section{Summary and Outlook}

This introduction has outlined the motivation and rationale for investigating neuronal identity and function through the dual lenses of input-dependence and neuromodulatory flexibility. Traditional classification approaches, though informative, fall short in capturing the high-dimensional, dynamic nature of neuronal computation. By leveraging state of the art patch clamp recordings done using frozen noise protocols, extracting high dimensional electrophysiological features from these recordings, and using modern unsupervised analysis techniques, this thesis proposes a data-driven framework to redefine neuronal identity as an emergent, context-sensitive construct. Finally, this thesis investigates the computational implications of intrinsic heterogeneity by modeling structured diversity in artificial reservoir systems. The following chapters present experimental results and theoretical insights that collectively support this paradigm shift.

\section{Thesis Structure}
\begin{itemize}
  \item \textbf{Chapter 2}: Neuronal Identity is Not Static: An Input-Driven Perspective \\
  Demonstrates how different stimulation protocols yield distinct classifications, showing that neuronal identity is dynamic and shaped by input.

  \item \textbf{Chapter 3}: Neuromodulatory Control of Cortical Function \\
  Examines how neuromodulators reshape the computational roles of neurons, altering both encoding capacity and feature interdependence.

  \item \textbf{Chapter 4}: Timescale heterogeneity in reservoir computing \\   
    Analyzes the effect of timescale heterogeneity on task performance of ESNs and DDNS, showing that moderate timescale heterogeneity improves the performance over networks without it.  

  \item \textbf{Chapter 5}: General Discussion and Future Directions \\
  Integrates findings across studies, discusses implications for neuroscience and artificial neural networks, and proposes directions for future research.
\end{itemize}


\newpage
\begin{spacing}{1.0} % Set the line spacing to single spacing
\fontsize{8pt}{8pt}\selectfont
\bibliographystyle{apalike}
\renewcommand{\bibname}{References}
\bibliography{All_bibtex}



\end{spacing}
